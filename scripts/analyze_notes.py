#!/usr/bin/env python3
"""
Obsidian vault ã®ãƒãƒ¼ãƒˆã‚’åˆ†æã—ã¦ã€ã‚¿ã‚°ã¨ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set


def extract_frontmatter(content: str) -> Dict[str, str]:
    """ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’æŠ½å‡º"""
    frontmatter = {}
    pattern = r'^---\s*\n(.*?)\n---\s*\n'
    match = re.search(pattern, content, re.DOTALL)

    if match:
        fm_content = match.group(1)
        for line in fm_content.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                frontmatter[key.strip()] = value.strip().strip('"')

    return frontmatter


def extract_tags(content: str) -> Set[str]:
    """ã‚¿ã‚°ã‚’æŠ½å‡ºï¼ˆ# ã§å§‹ã¾ã‚‹è¡Œã¨ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼å†…ã®ã‚¿ã‚°ï¼‰"""
    tags = set()

    # # ã§å§‹ã¾ã‚‹ã‚¿ã‚°ã‚’æŠ½å‡º
    tag_pattern = r'#(\w+(?:-\w+)*)'
    matches = re.findall(tag_pattern, content)
    tags.update(matches)

    return tags


def extract_links(content: str) -> List[str]:
    """åŒæ–¹å‘ãƒªãƒ³ã‚¯ [[...]] ã‚’æŠ½å‡º"""
    link_pattern = r'\[\[([^\]]+)\]\]'
    links = re.findall(link_pattern, content)
    return links


def analyze_file(file_path: Path) -> Dict:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦æƒ…å ±ã‚’è¿”ã™"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        frontmatter = extract_frontmatter(content)
        tags = extract_tags(content)
        links = extract_links(content)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤ã
        filename = file_path.stem

        return {
            'path': str(file_path.relative_to(Path.cwd())),
            'filename': filename,
            'title': frontmatter.get('title', filename),
            'created': frontmatter.get('created', ''),
            'updated': frontmatter.get('updated', ''),
            'tags': sorted(list(tags)),
            'links': links,
            'frontmatter': frontmatter
        }
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
        return None


def analyze_vault(base_dir: Path, exclude_dirs: List[str] = None) -> Dict[str, Dict]:
    """Vaultå…¨ä½“ã‚’åˆ†æ"""
    if exclude_dirs is None:
        exclude_dirs = ['daily', 'templates', '.git', '.obsidian']

    results = {}

    # log/ ã¨ data/ ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
    for directory in ['log', 'data']:
        dir_path = base_dir / directory
        if not dir_path.exists():
            continue

        for md_file in dir_path.glob('*.md'):
            file_info = analyze_file(md_file)
            if file_info:
                results[file_info['filename']] = file_info

    return results


def group_by_tags(results: Dict[str, Dict]) -> Dict[str, List[str]]:
    """ã‚¿ã‚°ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
    tag_groups = {}

    for filename, info in results.items():
        for tag in info['tags']:
            if tag not in tag_groups:
                tag_groups[tag] = []
            tag_groups[tag].append(filename)

    return tag_groups


def find_related_files(results: Dict[str, Dict], min_common_tags: int = 2) -> Dict[str, List[Dict]]:
    """é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆå…±é€šã‚¿ã‚°ãŒå¤šã„ã‚‚ã®ï¼‰"""
    related = {}

    for filename1, info1 in results.items():
        related[filename1] = []
        tags1 = set(info1['tags'])

        for filename2, info2 in results.items():
            if filename1 == filename2:
                continue

            tags2 = set(info2['tags'])
            common_tags = tags1 & tags2

            if len(common_tags) >= min_common_tags:
                related[filename1].append({
                    'filename': filename2,
                    'common_tags': sorted(list(common_tags)),
                    'score': len(common_tags)
                })

        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        related[filename1].sort(key=lambda x: x['score'], reverse=True)

    return related


def main():
    base_dir = Path.cwd()

    print("ğŸ“Š Vaultåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    results = analyze_vault(base_dir)

    print(f"\nâœ… {len(results)} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¾ã—ãŸ\n")

    # ã‚¿ã‚°ã”ã¨ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    tag_groups = group_by_tags(results)

    print("ğŸ“Œ ã‚¿ã‚°ã®ä¸€è¦§:")
    for tag, files in sorted(tag_groups.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"  #{tag}: {len(files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")

    # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
    print("\nğŸ”— é–¢é€£æ€§ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºä¸­...")
    related = find_related_files(results, min_common_tags=2)

    # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
    output_file = base_dir / 'scripts' / 'analysis_result.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'files': results,
            'tag_groups': tag_groups,
            'related': related
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

    # é–¢é€£æ€§ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ã‚‚ã®ï¼‰ã‚’è¡¨ç¤º
    print("\nğŸ¯ ãƒªãƒ³ã‚¯å€™è£œï¼ˆé–¢é€£æ€§ãŒé«˜ã„ãŒã¾ã ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰:")

    link_suggestions = []
    for filename, relations in related.items():
        if not relations:
            continue

        file_info = results[filename]
        existing_links = set(file_info['links'])

        for rel in relations[:5]:  # ä¸Šä½5ä»¶
            if rel['filename'] not in existing_links:
                link_suggestions.append({
                    'from': filename,
                    'to': rel['filename'],
                    'common_tags': rel['common_tags'],
                    'score': rel['score']
                })

    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    link_suggestions.sort(key=lambda x: x['score'], reverse=True)

    # ä¸Šä½20ä»¶ã‚’è¡¨ç¤º
    for i, suggestion in enumerate(link_suggestions[:20], 1):
        print(f"\n{i}. {suggestion['from']}")
        print(f"   â†’ {suggestion['to']}")
        print(f"   å…±é€šã‚¿ã‚°: {', '.join(f'#{tag}' for tag in suggestion['common_tags'])}")
        print(f"   ã‚¹ã‚³ã‚¢: {suggestion['score']}")


if __name__ == '__main__':
    main()
